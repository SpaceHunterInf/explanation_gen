{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.6270, -1.8796,  5.9538]], grad_fn=<AddmmBackward0>)\n",
      "Premise: Two women are embracing while holding to go packages.\n",
      "Hypothesis: The men are fighting outside a deli.\n",
      "Entailment: 6.901115557411686e-05\n",
      "Neutral: 0.0003960772883147001\n",
      "Contradiction: 0.9995349645614624\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig, RobertaForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    max_length = 256\n",
    "\n",
    "    premise = \"Two women are embracing while holding to go packages.\"\n",
    "    hypothesis = \"The men are fighting outside a deli.\"\n",
    "\n",
    "    hg_model_hub_name = \"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "    # hg_model_hub_name = \"ynie/albert-xxlarge-v2-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "    # hg_model_hub_name = \"ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "    # hg_model_hub_name = \"ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "    # hg_model_hub_name = \"ynie/xlnet-large-cased-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(hg_model_hub_name)\n",
    "    model = RobertaForSequenceClassification.from_pretrained(hg_model_hub_name)\n",
    "\n",
    "    tokenized_input_seq_pair = tokenizer.encode_plus(premise, hypothesis,\n",
    "                                                     max_length=max_length,\n",
    "                                                     return_token_type_ids=True, truncation=True)\n",
    "\n",
    "    input_ids = torch.Tensor(tokenized_input_seq_pair['input_ids']).long().unsqueeze(0)\n",
    "    # remember bart doesn't have 'token_type_ids', remove the line below if you are using bart.\n",
    "    token_type_ids = torch.Tensor(tokenized_input_seq_pair['token_type_ids']).long().unsqueeze(0)\n",
    "    attention_mask = torch.Tensor(tokenized_input_seq_pair['attention_mask']).long().unsqueeze(0)\n",
    "\n",
    "    outputs, encoding = model(input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    labels=None)\n",
    "    # Note:\n",
    "    # \"id2label\": {\n",
    "    #     \"0\": \"entailment\",\n",
    "    #     \"1\": \"neutral\",\n",
    "    #     \"2\": \"contradiction\"\n",
    "    # },\n",
    "    print(outputs[0])\n",
    "    predicted_probability = torch.softmax(outputs[0], dim=1)[0].tolist()  # batch_size only one\n",
    "\n",
    "    print(\"Premise:\", premise)\n",
    "    print(\"Hypothesis:\", hypothesis)\n",
    "    print(\"Entailment:\", predicted_probability[0])\n",
    "    print(\"Neutral:\", predicted_probability[1])\n",
    "    print(\"Contradiction:\", predicted_probability[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2462, -0.8286, -0.1720,  ..., -0.7325,  0.6001,  0.9092]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, encoding = model(input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    output_hidden_states=True,\n",
    "                    labels=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(outputs[0]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "matrix = None\n",
    "bias = None\n",
    "dense_m = None\n",
    "dense_b = None\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name == 'classifier.dense.weight':\n",
    "        dense_m = param\n",
    "    if name == 'classifier.dense.bias':\n",
    "        dense_b = param\n",
    "    if name == 'classifier.out_proj.weight':\n",
    "        matrix = param\n",
    "    if name == 'classifier.out_proj.bias':\n",
    "        bias = param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.6270, -1.8796,  5.9538]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(encoding, matrix.T) + bias\n",
    "#model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 23, 1024])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-3.6270, -1.8796,  5.9538]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden = outputs[1][-1][:,0,:]\n",
    "print(last_hidden.shape)\n",
    "dense_d = torch.mm(last_hidden, dense_m.T) + dense_b\n",
    "dense_d = torch.tanh(dense_d)\n",
    "dense_d.shape\n",
    "torch.mm(dense_d, matrix.T) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<mask>'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_masks(tokenized_text):\n",
    "    # https://stackoverflow.com/questions/1482308/how-to-get-all-subsets-of-a-set-powerset\n",
    "    # WITHOUT empty and full sets!\n",
    "    s = list(range(len(tokenized_text)))\n",
    "    x = len(s)\n",
    "    masks = [1 << i for i in range(x)]\n",
    "    #     for i in range(1 << x):  # empty and full sets included here\n",
    "    for i in range(1, 1 << x - 1):\n",
    "        yield [ss for mask, ss in zip(masks, s) if i & mask]\n",
    "        \n",
    "def all_consecutive_masks(tokenized_text, max_length = -1):\n",
    "    # WITHOUT empty and full sets!\n",
    "    s = list(range(len(tokenized_text)))\n",
    "    x = len(s)\n",
    "    for i in range(x):\n",
    "        for j in range(i+1, x):\n",
    "            mask = s[:i] + s[j:]\n",
    "            if max_length > 0:\n",
    "                if j - i >= max_length:\n",
    "                    yield mask\n",
    "            else:\n",
    "                yield mask\n",
    "                \n",
    "def all_consecutive_masks2(tokenized_text, max_length = -1):\n",
    "    # WITHOUT empty and full sets!\n",
    "    s = list(range(len(tokenized_text)))\n",
    "    x = len(s)\n",
    "    for i in range(x+1):\n",
    "        for j in range(i+1, x+1):\n",
    "            mask = s[i:j]\n",
    "            if max_length > 0:\n",
    "                if j - i <= max_length:\n",
    "                    yield mask\n",
    "            else:\n",
    "                yield mask\n",
    "\n",
    "def predict_json(ex, model, tokenizer):\n",
    "    premise = ex['premise']\n",
    "    hypothesis = ex['hypothesis']\n",
    "    tokenized_input_seq_pair = tokenizer.encode_plus(premise, hypothesis,\n",
    "                                                     max_length=max_length,\n",
    "                                                     return_token_type_ids=True, truncation=True)\n",
    "\n",
    "    input_ids = torch.Tensor(tokenized_input_seq_pair['input_ids']).long().unsqueeze(0)\n",
    "    # remember bart doesn't have 'token_type_ids', remove the line below if you are using bart.\n",
    "    token_type_ids = torch.Tensor(tokenized_input_seq_pair['token_type_ids']).long().unsqueeze(0)\n",
    "    attention_mask = torch.Tensor(tokenized_input_seq_pair['attention_mask']).long().unsqueeze(0)\n",
    "\n",
    "    outputs, encoding = model(input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    labels=None)\n",
    "    \n",
    "    label = model.config.id2label[torch.argmax(outputs[0]).item()]\n",
    "    \n",
    "    return {'encoded_representations': encoding, 'label':label}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  entailment\n",
      "{'premise': 'A soccer game in a large area with 8 yellow players and 4 black players.', 'hypothesis': '9 is a soccer game with 12 players.'}\n",
      "contradiction\n",
      "====\n",
      "{'premise': 'A soccer game in a large area with 8 yellow players and 4 black players.', 'hypothesis': 'There 9 a soccer game with 12 players.'}\n",
      "entailment\n",
      "====\n",
      "{'premise': 'A soccer game in a large area with 8 yellow players and 4 black players.', 'hypothesis': 'There is 9 soccer game with 12 players.'}\n",
      "contradiction\n",
      "====\n",
      "{'premise': 'A soccer game in a large area with 8 yellow players and 4 black players.', 'hypothesis': 'There is a 9 game with 12 players.'}\n",
      "contradiction\n",
      "====\n",
      "{'premise': 'A soccer game in a large area with 8 yellow players and 4 black players.', 'hypothesis': 'There is a soccer 9 with 12 players.'}\n",
      "contradiction\n",
      "====\n",
      "{'premise': 'A soccer game in a large area with 8 yellow players and 4 black players.', 'hypothesis': 'There is a soccer game 9 12 players.'}\n",
      "entailment\n",
      "====\n",
      "{'premise': 'A soccer game in a large area with 8 yellow players and 4 black players.', 'hypothesis': 'There is a soccer game with 9 players.'}\n",
      "entailment\n",
      "====\n",
      "{'premise': 'A soccer game in a large area with 8 yellow players and 4 black players.', 'hypothesis': 'There is a soccer game with 12 9'}\n",
      "contradiction\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ex = {'premise': 'A soccer game in a large area with 8 yellow players and 4 black players.', \n",
    "      'hypothesis': 'There is a soccer game with 12 players.', 'gold_label': 'entailment'}\n",
    "    \n",
    "foil = 'contradiction'#ex['gold_label']\n",
    "\n",
    "out = predict_json(ex, model, tokenizer)\n",
    "encoded_orig = out['encoded_representations']\n",
    "\n",
    "fact = out['label']\n",
    "print('Predicted: ', fact)\n",
    "\n",
    "# assert fact != foil, \"Fact should be different from the foil (if not, pick a different foil)\"\n",
    "\n",
    "ex['premise'] = ex['premise'].split()\n",
    "ex['hypothesis'] = ex['hypothesis'].split()\n",
    "\n",
    "#tokenizer.convert_tokens_to_string(out['tokens'])\n",
    "\n",
    "masks1 = [[]]  # change this if you also want to mask out parts of the premise.\n",
    "masks2 = list(all_consecutive_masks2(ex['hypothesis'], max_length=1))\n",
    "encoded = []\n",
    "mask_mapping = []\n",
    "preds = np.zeros(shape=(len(masks1), len(masks2)))\n",
    "\n",
    "for m1_i, m1 in enumerate(masks1):\n",
    "    masked1 = list(ex['premise'])\n",
    "    for i in m1:\n",
    "        masked1[i] = '<mask>'\n",
    "    masked1 = ' '.join(masked1)\n",
    "        \n",
    "    for m2_i, m2 in enumerate(masks2):\n",
    "        masked2 = list(ex['hypothesis'])\n",
    "        for i in m2:\n",
    "            masked2[i] = '9'\n",
    "        masked2 = ' '.join(masked2)\n",
    "            \n",
    "        masked_ex = {\n",
    "            \"premise\": masked1,\n",
    "            \"hypothesis\": masked2\n",
    "        }\n",
    "        \n",
    "        masked_out = predict_json(masked_ex, model, tokenizer)\n",
    "#         if masked_out['label'] != foil:\n",
    "#             continue\n",
    "        \n",
    "        #print(m1_i, m2_i)\n",
    "        #print(f\"{masked1}\\n{masked2}\")\n",
    "        print(masked_ex)\n",
    "        print(masked_out['label'])\n",
    "        encoded.append(masked_out['encoded_representations'].detach().numpy())\n",
    "        mask_mapping.append((m1_i, m2_i))\n",
    "        \n",
    "        print(\"====\")\n",
    "encoded = np.array(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact: entailment\n",
      "foil: contradiction\n",
      "(1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "foil = 'contradiction'\n",
    "label2index = {'entailment':0, 'neutral':1, 'contradiction':2}\n",
    "fact_idx = label2index[fact]\n",
    "foil_idx = label2index[foil]\n",
    "index2label = model.config.id2label\n",
    "print('fact:', index2label[fact_idx])\n",
    "print('foil:', index2label[foil_idx])\n",
    "num_classifiers = 100\n",
    "\n",
    "classifier_w = matrix.clone().detach().numpy()\n",
    "classifier_b = bias.clone().detach().numpy()\n",
    "\n",
    "u = classifier_w[fact_idx] - classifier_w[foil_idx]\n",
    "contrastive_projection = np.outer(u, u) / np.dot(u, u)\n",
    "\n",
    "print(contrastive_projection.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 3)\n",
      "(8, 3)\n",
      "A soccer game in a large area with 8 yellow players and 4 black players.\n",
      "There is a soccer game with 12 players.\n",
      "=========\n",
      "=======Farthest masks:=======\n",
      "A soccer game in a large area with 8 yellow players and 4 black players.\n",
      "9 is a soccer game with 12 players.\n",
      "0.3406\n",
      "A soccer game in a large area with 8 yellow players and 4 black players.\n",
      "There 9 a soccer game with 12 players.\n",
      "0.3406\n",
      "A soccer game in a large area with 8 yellow players and 4 black players.\n",
      "There is 9 soccer game with 12 players.\n",
      "0.3406\n",
      "A soccer game in a large area with 8 yellow players and 4 black players.\n",
      "There is a 9 game with 12 players.\n",
      "0.3406\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "z_all = encoded_orig \n",
    "z_h = encoded \n",
    "z_all_row = encoded_orig @ contrastive_projection\n",
    "z_h_row = encoded @ contrastive_projection\n",
    "\n",
    "prediction_probabilities = softmax(z_all_row @ classifier_w.T + classifier_b)\n",
    "prediction_probabilities = np.tile(prediction_probabilities, (z_h_row.shape[0], 1))\n",
    "\n",
    "prediction_probabilities_del = softmax(z_h_row @ classifier_w.T + classifier_b, axis=1).squeeze(1)\n",
    "print(prediction_probabilities.shape)\n",
    "print(prediction_probabilities_del.shape)\n",
    "p = prediction_probabilities[:, [fact_idx, foil_idx]]\n",
    "q = prediction_probabilities_del[:, [fact_idx, foil_idx]]\n",
    "\n",
    "p = p / p.sum(axis=1).reshape(-1, 1)\n",
    "q = q / q.sum(axis=1).reshape(-1, 1)\n",
    "distances = (p[:, 0] - q[:, 0])\n",
    "\n",
    "print(' '.join(ex['premise']))\n",
    "print(' '.join(ex['hypothesis']))\n",
    "\n",
    "print(\"=========\\n=======Farthest masks:=======\")    \n",
    "    \n",
    "highlight_rankings = np.argsort(-distances)\n",
    "\n",
    "for i in range(4):\n",
    "    rank = highlight_rankings[i]\n",
    "    m1_i, m2_i = mask_mapping[rank]\n",
    "    \n",
    "    masked1 = list(ex['premise'])\n",
    "    for k in masks1[m1_i]:\n",
    "        masked1[k] = '<mask>'\n",
    "    masked1 = ' '.join(masked1)\n",
    "    \n",
    "    masked2 = list(ex['hypothesis'])\n",
    "    for k in masks2[m2_i]:\n",
    "        masked2[k] = '<mask>'\n",
    "    masked2 = ' '.join(masked2)\n",
    "    \n",
    "    print(masked1)\n",
    "    print(masked2)\n",
    "    print(np.round(distances[rank], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contrastive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbf0fad223f40af2b57ef6954feeb395ca3306c455beb881a0fffbf073f7287e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
